% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ngr_s3_dl.R
\name{ngr_s3_dl}
\alias{ngr_s3_dl}
\title{Download Files from a Directory}
\usage{
ngr_s3_dl(url, path, glob = "\\\\.tif$", timeout_limit = 3600, ...)
}
\arguments{
\item{url}{\link{character} The URL of the directory containing files to download. This can be an FTP, HTTP, or S3 directory.}

\item{path}{\link{character} The local path where the files should be downloaded.}

\item{glob}{\link{character} A regular expression pattern to match file types. Default is \code{""} for downloading all files.
Can use options such as \code{"\\\\.tif$"} to download all \code{tif} files or \code{"11\\\\.tif$"} to download all tiff files with filenames
that end in 11.}

\item{timeout_limit}{\link{numeric} The timeout limit for file downloads in seconds. Default is \code{3600} (60 minutes).}

\item{...}{Empty. For passing arguments to \code{\link[curl:multi_download]{curl::multi_download()}}/}
}
\value{
A A tibble from \code{\link[curl:multi_download]{curl::multi_download()}} indicating the completion of the download process and the path to the downloaded files.
}
\description{
This function downloads files matching a specified pattern from a directory URL to a local folder.
}
\details{
The function parses the HTML page at the provided URL, identifies links matching the specified pattern, and downloads each file to the specified local directory. Note that this will only work for S3 buckets exposed as static websites (e.g., \verb{http://example-bucket.s3-website-region.amazonaws.com}).
}
\examples{
# \dontrun{
# Download all .tiff and .pdf files from the main directory of a static S3 bucket
url <- "http://example-bucket.s3-website-region.amazonaws.com"
path <- "./downloaded_files"
glob <- "\\\\.(tiff|pdf)$"
ngr_s3_dl(url, path, glob)
# }
}
